
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>9.1. Lecture 22 &#8212; Learning from data</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"N": ["\\mathbb{N}"], "Z": ["\\mathbb{Z}"], "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"], "alphavec": ["\\boldsymbol{\\alpha}"], "muvec": ["\\boldsymbol{\\mu}"], "phivec": ["\\boldsymbol{\\phi}"], "sigmavec": ["\\boldsymbol{\\sigma}"], "Sigmavec": ["\\boldsymbol{\\Sigma}"], "thetavec": ["\\boldsymbol{\\theta}"], "thetavechat": ["\\widehat\\thetavec"], "avec": ["\\boldsymbol{a}"], "Bvec": ["\\boldsymbol{B}"], "fvec": ["\\boldsymbol{f}"], "mvec": ["\\boldsymbol{m}"], "qvec": ["\\boldsymbol{q}"], "rvec": ["\\boldsymbol{r}"], "uvec": ["\\boldsymbol{u}"], "wvec": ["\\boldsymbol{w}"], "xvec": ["\\boldsymbol{x}"], "yvec": ["\\boldsymbol{y}"], "Lra": ["\\Longrightarrow"], "abar": ["\\overline a"], "Xbar": ["\\overline X"], "alphahat": ["\\widehat\\alpha"], "Hhat": ["\\hat H"], "yth": ["y_{\\text{th}}"], "yexp": ["y_{\\text{exp}}"], "ym": ["y_{\\text{m}}"], "gs": ["{0}"]}}})</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="9.2. Bayesian Optimization" href="../../notebooks/Machine_learning/Bayesian_optimization.html" />
    <link rel="prev" title="9. Machine learning" href="machine_learning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/8820_icon.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning from data</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../about.html">
   About this Jupyter Book
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Course overview
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Course/overview.html">
   Objectives
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Topics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Basics/basics.html">
   1. Basics of Bayesian statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/lecture_01.html">
     1.1. Lecture 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Basics/Exploring_pdfs.html">
     1.2. Exploring PDFs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Basics/simple_sum_product_rule.html">
     1.3. Checking the sum and product rules, and their consequences
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/lecture_02.html">
     1.4. Lecture 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Basics/Bayesian_updating_coinflip_interactive.html">
     1.5. Interactive Bayesian updating: coin flipping example
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Basics/medical_example_by_Bayes.html">
     1.6. Standard medical example by applying Bayesian rules of probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Basics/radioactive_lighthouse_exercise.html">
     1.7. Radioactive lighthouse problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/lecture_03.html">
     1.8. Lecture 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Parameter_estimation/param_est.html">
   2. Bayesian parameter estimation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/lecture_04.html">
     2.1. Lecture 4: Parameter estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Parameter_estimation/parameter_estimation_Gaussian_noise.html">
     2.2. Parameter estimation example: Gaussian noise and averages
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/assignments/Assignment_extending_radioactive_lighthouse.html">
     2.3. Assignment: 2D radioactive lighthouse location using MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/lecture_05.html">
     2.4. Lecture 5
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Parameter_estimation/parameter_estimation_fitting_straight_line_I.html">
     2.5. Parameter estimation example: fitting a straight line
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Parameter_estimation/demo-ModelValidation.html">
     2.6. Linear Regression and Model Validation demonstration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/lecture_06.html">
     2.7. Lecture 6
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Parameter_estimation/amplitude_in_presence_of_background.html">
     2.8. Amplitude of a signal in the presence of background
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/assignments/Assignment_parameter_estimation_followups.html">
     2.9. Assignment: Follow-ups to Parameter Estimation notebooks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Parameter_estimation/exercise_LinearRegression.html">
     2.10. Linear Regression exercise
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Parameter_estimation/linear_algebra_games_I.html">
     2.11. Linear algebra games including SVD for PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/assignments/fluctuation_trend_with_number_of_points_and_data_errors.html">
     2.12. Follow-up: fluctuation trends with # of points and data errors
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../MCMC_sampling_I/MCMC_sampling_I.html">
   3. MCMC sampling I
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_I/lecture_07.html">
     3.1. Lecture 7
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_I/Metropolis_Poisson_example.html">
     3.2. Metropolis-Hasting MCMC sampling of a Poisson distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_I/lecture_08.html">
     3.3. Lecture 8
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_I/MCMC-random-walk-and-sampling.html">
     3.4. Exercise: Random walk
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Why_Bayes_is_better/bayes_is_better.html">
   4. Why Bayes is better
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/lecture_09.html">
     4.1. Lecture 9
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Why_Bayes_is_better/bayes_billiard.html">
     4.2. A Bayesian Billiard game
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/lecture_10.html">
     4.3. Lecture 10
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Why_Bayes_is_better/parameter_estimation_fitting_straight_line_II.html">
     4.4. Parameter estimation example: fitting a straight line II
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/lecture_11.html">
     4.5. Lecture 11
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Why_Bayes_is_better/error_propagation_to_functions_of_uncertain_parameters.html">
     4.6. Error propagation: Example 3.6.2 in Sivia
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Basics/visualization_of_CLT.html">
     4.7. Visualization of the Central Limit Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Basics/correlation_intuition.html">
     4.8. Building intuition about correlations (and a bit of Python linear algebra)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/lecture_12.html">
     4.9. Lecture 12
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_I/MCMC-diagnostics.html">
     4.10. Overview: MCMC Diagnostics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/lecture_13.html">
     4.12. Lecture 13
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Why_Bayes_is_better/dealing_with_outliers.html">
     4.13. Dealing with outliers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Model_selection/model_selection.html">
   5. Model selection
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Model_selection/lecture_14.html">
     5.1. Lecture 14
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Model_selection/lecture_15.html">
     5.2. Lecture 15
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Model_selection/Evidence_for_model_EFT_coefficients.html">
     5.3. Evidence calculation for EFT expansions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Model_selection/lecture_16.html">
     5.4. Lecture 16
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/mini-projects/MCMC-parallel-tempering_ptemcee.html">
     5.5. Example: Parallel tempering for multimodal distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/mini-projects/MCMC-parallel-tempering_ptemcee_vs_zeus.html">
     5.6. Example: Parallel tempering for multimodal distributions vs. zeus
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../MCMC_sampling_II/MCMC_sampling_II.html">
   6. MCMC sampling II
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/lecture_17.html">
     6.1. Lecture 17
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_II/chi_squared_tests.html">
     6.2. Quick check of the distribution of normal variables squared
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_II/Liouville_theorem_visualization.html">
     6.3. Liouville Theorem Visualization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_II/Orbital_eqs_with_different_algorithms.html">
     6.4. Solving orbital equations with different algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/lecture_18.html">
     6.5. Lecture 18
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_II/PyMC3_intro_updated.html">
     6.6. PyMC3 Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_II/PyMC3_docs_getting_started_updated.html">
     6.7. Getting started with PyMC3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Parameter_estimation/parameter_estimation_Gaussian_noise_compare_samplers.html">
     6.8. Comparing samplers for a simple problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/mini-projects/zeus_multimodal.html">
     6.9. zeus: Sampling from multimodal distributions
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Gaussian_processes/gaussian_processes.html">
   7. Gaussian processes
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Gaussian_processes/lecture_19.html">
     7.1. Lecture 19
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Gaussian_processes/demo-GaussianProcesses.html">
     7.2. Gaussian processes demonstration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Gaussian_processes/GaussianProcesses.html">
     7.3. Learning from data: Gaussian processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Gaussian_processes/Gaussian_processes_exercises.html">
     7.4. Exercise: Gaussian Process models with GPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Gaussian_processes/lecture_20.html">
     7.5. Lecture 20
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Maximum_entropy/max_ent.html">
   8. Assigning probabilities
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Maximum_entropy/lecture_21.html">
     8.1. Lecture 21
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Maximum_entropy/MaxEnt.html">
     8.2. Ignorance pdfs: Indifference and translation groups
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Maximum_entropy/Pdfs_from_MaxEnt.html">
     8.3. MaxEnt for deriving some probability distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Maximum_entropy/MaxEnt_Function_Reconstruction.html">
     8.4. Maximum Entropy for reconstructing a function from its moments
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Maximum_entropy/demo-MaxEnt.html">
     8.5. Making figures for Ignorance PDF notebook
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="machine_learning.html">
   9. Machine learning: Bayesian methods
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     9.1. Lecture 22
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Machine_learning/Bayesian_optimization.html">
     9.2. Bayesian Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lecture_23.html">
     9.3. Lecture 23
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Machine_learning/Neural_networks_explained.html">
     9.4. What Are Neural Networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Machine_learning/Forssen_tif285_NeuralNet.html">
     9.5. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Machine_learning/Forssen_tif285_demo-NeuralNet.html">
     9.6. Neural network classifier demonstration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Machine_learning/Bayesian_neural_networks_tif285.html">
     9.7. Bayesian neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lecture_24.html">
     9.8. Lecture 24
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Machine_learning/demo-Bayesian_neural_networks_tif285.html">
     9.9. Variational Inference: Bayesian Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Machine_learning/Convolutional_neural_network_explained.html">
     9.10. What is a convolutional neural network?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../SVD/svd.html">
   10. PCA, SVD, and all that
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/SVD/linear_algebra_games_including_SVD.html">
     10.1. Linear algebra games including SVD for PCA
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Mini-projects
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/mini-projects/mini-project_I_toy_model_of_EFT.html">
   Mini-project I: Parameter estimation for a toy model of an EFT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/mini-projects/model-selection_mini-project-IIa.html">
   Mini-project IIa: Model selection basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/mini-projects/model-selection_mini-project-IIb_How_many_lines_ptemcee.html">
   Mini-project IIb: How many lines are there?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/mini-projects/mini-project_IIIa_bayesian_optimization.html">
   Mini-project IIIa: Bayesian optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/mini-projects/mini-project_IIIb_Bayesian_neural_networks_from_demo.html">
   Mini-project IIIb: Bayesian Neural Networks
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reference material
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../zbibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../related_topics.html">
   Related topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Reference/installing_anaconda.html">
   Using Anaconda
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Reference/using_github.html">
   Using GitHub
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Reference/python_jupyter.html">
   Python and Jupyter notebooks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Reference/Jupyter_Python_intro_01.html">
     Python and Jupyter notebooks: part 01
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Reference/Jupyter_Python_intro_02.html">
     Python and Jupyter notebooks: part 02
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../jb_tests.html">
   Examples: Jupyter jb-book
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Notebook keys
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/Basics/simple_sum_product_rule_KEY.html">
   Checking the sum and product rules, and their consequences
   <span style="color: red">
    Key
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/Basics/medical_example_by_Bayes_KEY.html">
   Standard medical example by applying Bayesian rules of probability
   <span style="color: red">
    Key
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/Basics/radioactive_lighthouse_exercise_key.html">
   Radioactive lighthouse problem
   <span style="color: red">
    Key
   </span>
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/Machine_learning/lecture_22.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/furnstahl/Physics-8820"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/furnstahl/Physics-8820/issues/new?title=Issue%20on%20page%20%2Fcontent/Machine_learning/lecture_22.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maxent-function-reconstruction">
   MaxEnt function reconstruction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#another-inverse-problem-response-functions-from-euclidean-correlators">
     Another inverse problem: response functions from Euclidean correlators
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-methods-in-machine-learning-ml-background-remarks">
   Bayesian methods in machine learning (ML): background remarks
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="lecture-22">
<h1><span class="section-number">9.1. </span>Lecture 22<a class="headerlink" href="#lecture-22" title="Permalink to this headline">¶</a></h1>
<div class="section" id="maxent-function-reconstruction">
<h2>MaxEnt function reconstruction<a class="headerlink" href="#maxent-function-reconstruction" title="Permalink to this headline">¶</a></h2>
<p>Quick summary of <a class="reference internal" href="../../notebooks/Maximum_entropy/MaxEnt_Function_Reconstruction.html"><span class="doc std std-doc">Maximum Entropy for reconstructing a function from its moments</span></a>:</p>
<ul>
<li><p>Consider a function <span class="math notranslate nohighlight">\(f(x)\)</span> on <span class="math notranslate nohighlight">\(x \in [0,1]\)</span> with <span class="math notranslate nohighlight">\(f(x)&gt;0\)</span>, i.e., like a probability distribution, and without loss of generality we can assume that <span class="math notranslate nohighlight">\(f(x)\)</span> is normalized (just an overall constant).
Suppose we know <em>moments</em> <span class="math notranslate nohighlight">\(\mu_j\)</span> of the function:</p>
<div class="math notranslate nohighlight" id="equation-eq-mu-j-integrals">
<span class="eqno">(9.1)<a class="headerlink" href="#equation-eq-mu-j-integrals" title="Permalink to this equation">¶</a></span>\[
      \mu_j = \int_0^1 dx \, x^j f(x), \quad j=0,\ldots,N .
    \]</div>
<p>Our goal is to best reconstruct <span class="math notranslate nohighlight">\(f(x)\)</span> from the <span class="math notranslate nohighlight">\(N+1\)</span> moments.
This seems problematic as there are an infinite number of different functions that share that set of moments.
Here we use maximum entropy as a prescription for the construction of a sequence of approximations, one for each <span class="math notranslate nohighlight">\(N\)</span>, which converges to <span class="math notranslate nohighlight">\(f(x)\)</span> as <span class="math notranslate nohighlight">\(N\rightarrow \infty\)</span>.</p>
</li>
<li><p>As before, we define the entropy functional</p>
<div class="math notranslate nohighlight">
\[
    S[f] = -\int_0^1 dx\, \bigl(f(x)\log f(x) - f(x) \bigr)
     + \sum_{j=0}^N \lambda_j \left(\int_{0}^{1} dx\, x^j f(x)  - \mu_j\right) ,
    \]</div>
<p>where the Lagrange multipliers <span class="math notranslate nohighlight">\(\lambda_j\)</span> enable us to maximize the entropy subject to the constraints given by the moments.
The usual argument is that the maximum entropy approximation is the least biased.</p>
</li>
<li><p>Mead and Papanicolaou, <a class="reference external" href="https://bayes.wustl.edu/Manual/MeadPapanicolaou.pdf">J. Math. Phys. 24, 2404 (1984)</a>, formulated a solution for this problem; it is implemented in the notebook. They also prove many theorems.</p></li>
<li><p>The starting point is to vary <span class="math notranslate nohighlight">\(S[f]\)</span> with respect to the unknown <span class="math notranslate nohighlight">\(f(x)\)</span> and set it equal to zero. This yields</p>
<div class="math notranslate nohighlight">
\[
       f(x) = f_N(x) = e^{-\lambda_0 - \sum_{j=1}^N \lambda_j x^j}
    \]</div>
<p>along with the conditions on the moments <span class="math notranslate nohighlight">\(\mu_j\)</span> given in Eq. <a class="reference internal" href="#equation-eq-mu-j-integrals">(9.1)</a>.</p>
</li>
<li><p>Then one uses <span class="math notranslate nohighlight">\(\int_0^1 dx\, f_N(x) = 1\)</span> to express <span class="math notranslate nohighlight">\(\lambda_0\)</span> in terms of the remaining Lagrange multipliers and define the “partition function” <span class="math notranslate nohighlight">\(Z\)</span>:</p>
<div class="math notranslate nohighlight">
\[
      e^{\lambda_0} = \int_0^1 dx\, e^{- \sum_{j=1}^N \lambda_j x^j}
        \equiv Z .
    \]</div>
</li>
<li><p>The basic idea from here is to make use of the statistical mechanics analogy and introduce a potential <span class="math notranslate nohighlight">\(\Lambda = \Lambda(\lambda_1,\ldots,\lambda_N)\)</span> via the Legendre transformation</p>
<div class="math notranslate nohighlight">
\[
       \Lambda = \log Z + \sum_{n=1}^N \mu_n\lambda_n ,
    \]</div>
<p>where the <span class="math notranslate nohighlight">\(\mu_n\)</span> here are the actual numerical values of the known moments.</p>
<ul class="simple">
<li><p>Then <span class="math notranslate nohighlight">\(\Lambda\)</span> is convex everywhere for arbitrary <span class="math notranslate nohighlight">\(\mu_j\)</span> and its stationary points with respect to the <span class="math notranslate nohighlight">\(\lambda_j\)</span> are the solutions of the moment equations.
We can use a numerical minimization of the potential to find the desired result.</p></li>
<li><p>A necessary and sufficient condition on the moments that the potential <span class="math notranslate nohighlight">\(\Lambda\)</span> has a unique absolute minimum at finite <span class="math notranslate nohighlight">\(\lambda_j\)</span> for any <span class="math notranslate nohighlight">\(N\)</span> is that the <span class="math notranslate nohighlight">\(\{\mu_j: j=0, \ldots, N\}\)</span> must be “completely monotonic” (see the paper for details).</p></li>
</ul>
</li>
<li><p>The <span class="math notranslate nohighlight">\(N=1\)</span> case has</p>
<div class="math notranslate nohighlight">
\[
      \Lambda = \log[(1 - e^{-\lambda_1})/\lambda_1] + \mu_1\lambda_1 .
    \]</div>
<p>This only possesses a minimum if <span class="math notranslate nohighlight">\( \mu_1 &lt; \mu_0 = 1\)</span>.</p>
</li>
<li><p>In the notebook it is interesting to see the approximation of polynomials by exponentials of polynomials!</p></li>
</ul>
<div class="section" id="another-inverse-problem-response-functions-from-euclidean-correlators">
<h3>Another inverse problem: response functions from Euclidean correlators<a class="headerlink" href="#another-inverse-problem-response-functions-from-euclidean-correlators" title="Permalink to this headline">¶</a></h3>
<p>Another type of inverse problem is the determination of response funtions from theoretical calculations of imaginary-time (Euclidean) correlation functions, as obtained from quantum Monte Carlo simulations (for example). An example from nuclear physics is the microscopic description of the nuclear response to external electroweak probes.
Nuclear quantum Monte Carlo methods generate the imaginary-time correlators, with the response following from an inverse Laplace transform.
However, such inversions are notorious for being ill-posed problems.</p>
<p>The theorist calculates:</p>
<div class="math notranslate nohighlight">
\[
 \frac{E_{\alpha\beta}(q, \tau)}{C_{\alpha\beta}(\tau)}
  = \frac{\langle\gs |O_\alpha^\dagger(\qvec) e^{-(H - E_0)\tau} O_\beta(\qvec) | \gs \rangle}
  {\langle\gs | e^{-(H - E_0)\tau} | \gs \rangle}
\]</div>
<p>The experimentalist measures (the sum is over final states):</p>
<div class="math notranslate nohighlight">
\[
  R_{\alpha\beta}(q,\omega) \propto
   \sum_{f}\delta(\omega + E_0 - E_{f})
    \langle \gs | O_\alpha(\qvec) | f \rangle 
    \langle f | O_\beta(\qvec) | \gs \rangle .
\]</div>
<p>These are related by</p>
<div class="math notranslate nohighlight">
\[
  E_{\alpha\beta}(q, \tau) = C_{\alpha\beta}(\tau)
  \int_{\omega_{\rm th}}^{\infty} d\omega\, e^{-\tau\omega}
  R_{\alpha\beta}(q,\omega) .
\]</div>
<div class="admonition-why-not-just-invert-the-laplace-transform admonition">
<p class="admonition-title">Why not just invert the Laplace transform?</p>
<p>At large positive frequencies the kernel is exponentially small, so large <span class="math notranslate nohighlight">\(\omega\)</span> features of <span class="math notranslate nohighlight">\(R(\omega)\)</span> dpend on subtle features of <span class="math notranslate nohighlight">\(E(\tau)\)</span>.</p>
</div>
<p>One solution is to apply Bayesian methods, such as maximum entropy.
A nuclear example is in <a class="reference external" href="https://arxiv.org/abs/1501.01981"><em>Electromagnetic and neutral-weak response functions of 4He and 12C</em></a> by Lovato et al.
More recently, machine learning methods have been used to provide more robust inversions than maximum entropy. See <a class="reference external" href="https://arxiv.org/abs/2010.12703"><em>Machine learning-based inversion of nuclear responses</em></a> by Raghavan et al. for details and a comparison to the MaxEnt results.</p>
</div>
</div>
<div class="section" id="bayesian-methods-in-machine-learning-ml-background-remarks">
<h2>Bayesian methods in machine learning (ML): background remarks<a class="headerlink" href="#bayesian-methods-in-machine-learning-ml-background-remarks" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p>ML encompasses a broad array of techniques, many of which do not require a Bayesian approach, or they may even have a philosophy largely counter to the Bayes way of doing things.</p>
<ul class="simple">
<li><p>But there are clearly places where Bayesian methods are useful.</p></li>
<li><p>We will tough upon two examples:</p>
<ol class="simple">
<li><p>Bayesian Optimization</p></li>
<li><p>Bayesian Neural Networks</p></li>
</ol>
</li>
<li><p>Given time limitations, these will necessarily only be teasers for a more complete treatment.</p></li>
</ul>
</li>
<li><p>Let’s step through the Neal_Bayesian_Methods_for_Machine_Learning_selected_slides. Some comments:</p>
<ol>
<li><p>These came from 2004, which seems out-of-date, but the underlying ML ideas have been around for a long time. Recent successes have stemmed from refinements of old ideas (some of which were thought not to work, but just needed implementation tweaks).</p></li>
<li><p>Bayesian Approach to ML (or anything!).</p>
<ul class="simple">
<li><p>Emphasis that the approach is very general.</p></li>
<li><p>Some of the uses in step 4) are <em>often</em> different in ML.</p></li>
<li><p>Often in ML one doesn’t account for uncertainty, and optimize to find predictions. But some applications require an assessment of risk (e.g., medical applications) or a non-black-box idea of how the conclusion was reached (e.g., legal applications).</p></li>
</ul>
</li>
<li><p>Distinctive features set up to contrast with black-box ML.</p></li>
<li><p>“Learning Machine” approach is <em>one way</em> to view ML.</p>
<ul class="simple">
<li><p>Note that is works best with “big data”, i.e., a log of data, in which case we know that Bayesian priors become less important.</p></li>
<li><p>Conversely, the relevant of a Bayesian approach is greater when data is limited (or expensive to get).</p></li>
</ul>
</li>
<li><p>Challenge of Specifying Models and Priors</p>
<ul class="simple">
<li><p>Emphasis is on hierarchical models (i.e., those with hyperparameters) and iteractive approaches.</p></li>
</ul>
</li>
<li><p>Computational Challenge</p>
<ul class="simple">
<li><p>We’ve seen all of these except Variational approximation <span class="math notranslate nohighlight">\(\Lra\)</span> we’ll try an example for ML in Mini-project IIIb on Bayesian neural networs (BNNs).</p></li>
<li><p>Recall variational methods in physics, which can be powerful and computationally efficient: given an <em>approximate</em> wavefunction, with parameters controlling its form, an upper bound to the energy is found by taking the expectation value of the Hamiltonian with the wavefunction. Adjusting parameters to lower the energy also gives a better wavefunction.</p></li>
<li><p>Now replace the wavefunction by the target posterior.</p></li>
</ul>
</li>
<li><p>Multilayer Perceptron Neutral Networks</p>
<ul>
<li><p>There are various types of neural networks in use, with different strengths. They vary in connectivity, whether signals feed forward only or feed back, etc.</p></li>
<li><p>The schematic picture is</p>
<a class="bg-primary reference internal image-reference" href="../../_images/schematic_perceptron_NN_handdrawn.png"><img alt="schematic neural network" class="bg-primary align-center" src="../../_images/schematic_perceptron_NN_handdrawn.png" style="width: 300px;" /></a>
<p>Train with a set of inputs and outputs (supervised     learning), which determines the weights that dictate how     inputs are mapped to outputs.
Unsupervised learning has inputs and a cost function to     be minimized.</p>
</li>
<li><p>This is a very versatile way to reproduce functions.</p></li>
<li><p>ML is used for classification (what numeral is that? what phase? cat or not?) and regression (learn a function).</p></li>
<li><p>Inputs <span class="math notranslate nohighlight">\(x_1,\ldots, x_p\)</span> depend on details of the problem.</p></li>
<li><p>One node:</p>
<a class="bg-primary reference internal image-reference" href="../../_images/schematic_perceptron_input_handdrawn.png"><img alt="schematic neural network node" class="bg-primary align-center" src="../../_images/schematic_perceptron_input_handdrawn.png" style="width: 500px;" /></a>
<p>In the example one node would be one <span class="math notranslate nohighlight">\(j\)</span>. Then <span class="math notranslate nohighlight">\(U_{ij} \rightarrow W_i\)</span>, <span class="math notranslate nohighlight">\(\tanh\)</span> is the AF, and we only consider one output.</p>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "furnstahl/Physics-8820",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/Machine_learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="machine_learning.html" title="previous page"><span class="section-number">9. </span>Machine learning</a>
    <a class='right-next' id="next-link" href="../../notebooks/Machine_learning/Bayesian_optimization.html" title="next page"><span class="section-number">9.2. </span>Bayesian Optimization</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dick Furnstahl<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>