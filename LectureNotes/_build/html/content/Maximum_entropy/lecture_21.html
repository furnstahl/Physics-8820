
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>8.1. Lecture 21 &#8212; Learning from data</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"N": ["\\mathbb{N}"], "Z": ["\\mathbb{Z}"], "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"], "alphavec": ["\\boldsymbol{\\alpha}"], "muvec": ["\\boldsymbol{\\mu}"], "phivec": ["\\boldsymbol{\\phi}"], "sigmavec": ["\\boldsymbol{\\sigma}"], "Sigmavec": ["\\boldsymbol{\\Sigma}"], "thetavec": ["\\boldsymbol{\\theta}"], "thetavechat": ["\\widehat\\thetavec"], "avec": ["\\boldsymbol{a}"], "Bvec": ["\\boldsymbol{B}"], "fvec": ["\\boldsymbol{f}"], "mvec": ["\\boldsymbol{m}"], "qvec": ["\\boldsymbol{q}"], "rvec": ["\\boldsymbol{r}"], "uvec": ["\\boldsymbol{u}"], "wvec": ["\\boldsymbol{w}"], "xvec": ["\\boldsymbol{x}"], "yvec": ["\\boldsymbol{y}"], "Lra": ["\\Longrightarrow"], "abar": ["\\overline a"], "Xbar": ["\\overline X"], "alphahat": ["\\widehat\\alpha"], "Hhat": ["\\hat H"], "yth": ["y_{\\text{th}}"], "yexp": ["y_{\\text{exp}}"], "ym": ["y_{\\text{m}}"], "gs": ["{0}"]}}})</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="8.2. Ignorance pdfs: Indifference and translation groups" href="../../notebooks/Maximum_entropy/MaxEnt.html" />
    <link rel="prev" title="8. Maximum entropy" href="max_ent.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/8820_icon.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning from data</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../about.html">
   About this Jupyter Book
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Course overview
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Course/overview.html">
   Objectives
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Topics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Basics/basics.html">
   1. Basics of Bayesian statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/lecture_01.html">
     1.1. Lecture 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Basics/Exploring_pdfs.html">
     1.2. Exploring PDFs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Basics/simple_sum_product_rule.html">
     1.3. Checking the sum and product rules, and their consequences
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/lecture_02.html">
     1.4. Lecture 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Basics/Bayesian_updating_coinflip_interactive.html">
     1.5. Interactive Bayesian updating: coin flipping example
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Basics/medical_example_by_Bayes.html">
     1.6. Standard medical example by applying Bayesian rules of probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Basics/radioactive_lighthouse_exercise.html">
     1.7. Radioactive lighthouse problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/lecture_03.html">
     1.8. Lecture 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Parameter_estimation/param_est.html">
   2. Bayesian parameter estimation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/lecture_04.html">
     2.1. Lecture 4: Parameter estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Parameter_estimation/parameter_estimation_Gaussian_noise.html">
     2.2. Parameter estimation example: Gaussian noise and averages
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/assignments/Assignment_extending_radioactive_lighthouse.html">
     2.3. Assignment: 2D radioactive lighthouse location using MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/lecture_05.html">
     2.4. Lecture 5
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Parameter_estimation/parameter_estimation_fitting_straight_line_I.html">
     2.5. Parameter estimation example: fitting a straight line
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Parameter_estimation/demo-ModelValidation.html">
     2.6. Linear Regression and Model Validation demonstration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/lecture_06.html">
     2.7. Lecture 6
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Parameter_estimation/amplitude_in_presence_of_background.html">
     2.8. Amplitude of a signal in the presence of background
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/assignments/Assignment_parameter_estimation_followups.html">
     2.9. Assignment: Follow-ups to Parameter Estimation notebooks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Parameter_estimation/exercise_LinearRegression.html">
     2.10. Linear Regression exercise
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Parameter_estimation/linear_algebra_games_I.html">
     2.11. Linear algebra games including SVD for PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/assignments/fluctuation_trend_with_number_of_points_and_data_errors.html">
     2.12. Follow-up: fluctuation trends with # of points and data errors
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../MCMC_sampling_I/MCMC_sampling_I.html">
   3. MCMC sampling I
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_I/lecture_07.html">
     3.1. Lecture 7
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_I/Metropolis_Poisson_example.html">
     3.2. Metropolis-Hasting MCMC sampling of a Poisson distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_I/lecture_08.html">
     3.3. Lecture 8
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_I/MCMC-random-walk-and-sampling.html">
     3.4. Exercise: Random walk
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Why_Bayes_is_better/bayes_is_better.html">
   4. Why Bayes is better
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/lecture_09.html">
     4.1. Lecture 9
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Why_Bayes_is_better/bayes_billiard.html">
     4.2. A Bayesian Billiard game
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/lecture_10.html">
     4.3. Lecture 10
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Why_Bayes_is_better/parameter_estimation_fitting_straight_line_II.html">
     4.4. Parameter estimation example: fitting a straight line II
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/lecture_11.html">
     4.5. Lecture 11
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Why_Bayes_is_better/error_propagation_to_functions_of_uncertain_parameters.html">
     4.6. Error propagation: Example 3.6.2 in Sivia
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Basics/visualization_of_CLT.html">
     4.7. Visualization of the Central Limit Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Basics/correlation_intuition.html">
     4.8. Building intuition about correlations (and a bit of Python linear algebra)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/lecture_12.html">
     4.9. Lecture 12
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_I/MCMC-diagnostics.html">
     4.10. Overview: MCMC Diagnostics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/lecture_13.html">
     4.12. Lecture 13
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Why_Bayes_is_better/dealing_with_outliers.html">
     4.13. Dealing with outliers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Model_selection/model_selection.html">
   5. Model selection
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Model_selection/lecture_14.html">
     5.1. Lecture 14
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Model_selection/lecture_15.html">
     5.2. Lecture 15
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Model_selection/Evidence_for_model_EFT_coefficients.html">
     5.3. Evidence calculation for EFT expansions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Model_selection/lecture_16.html">
     5.4. Lecture 16
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/mini-projects/MCMC-parallel-tempering_ptemcee.html">
     5.5. Example: Parallel tempering for multimodal distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/mini-projects/MCMC-parallel-tempering_ptemcee_vs_zeus.html">
     5.6. Example: Parallel tempering for multimodal distributions vs. zeus
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../MCMC_sampling_II/MCMC_sampling_II.html">
   6. MCMC sampling II
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/lecture_17.html">
     6.1. Lecture 17
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_II/chi_squared_tests.html">
     6.2. Quick check of the distribution of normal variables squared
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_II/Liouville_theorem_visualization.html">
     6.3. Liouville Theorem Visualization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_II/Orbital_eqs_with_different_algorithms.html">
     6.4. Solving orbital equations with different algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/lecture_18.html">
     6.5. Lecture 18
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_II/PyMC3_intro_updated.html">
     6.6. PyMC3 Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/MCMC_sampling_II/PyMC3_docs_getting_started_updated.html">
     6.7. Getting started with PyMC3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Parameter_estimation/parameter_estimation_Gaussian_noise_compare_samplers.html">
     6.8. Comparing samplers for a simple problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/mini-projects/zeus_multimodal.html">
     6.9. zeus: Sampling from multimodal distributions
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Gaussian_processes/gaussian_processes.html">
   7. Gaussian processes
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Gaussian_processes/lecture_19.html">
     7.1. Lecture 19
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Gaussian_processes/demo-GaussianProcesses.html">
     7.2. Gaussian processes demonstration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Gaussian_processes/GaussianProcesses.html">
     7.3. Learning from data: Gaussian processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Gaussian_processes/Gaussian_processes_exercises.html">
     7.4. Exercise: Gaussian Process models with GPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Gaussian_processes/lecture_20.html">
     7.5. Lecture 20
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="max_ent.html">
   8. Assigning probabilities
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     8.1. Lecture 21
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Maximum_entropy/MaxEnt.html">
     8.2. Ignorance pdfs: Indifference and translation groups
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Maximum_entropy/Pdfs_from_MaxEnt.html">
     8.3. MaxEnt for deriving some probability distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Maximum_entropy/MaxEnt_Function_Reconstruction.html">
     8.4. Maximum Entropy for reconstructing a function from its moments
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Maximum_entropy/demo-MaxEnt.html">
     8.5. Making figures for Ignorance PDF notebook
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Machine_learning/machine_learning.html">
   9. Machine learning: Bayesian methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Machine_learning/lecture_22.html">
     9.1. Lecture 22
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Machine_learning/Bayesian_optimization.html">
     9.2. Bayesian Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Machine_learning/lecture_23.html">
     9.3. Lecture 23
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Machine_learning/Neural_networks_explained.html">
     9.4. What Are Neural Networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Machine_learning/Forssen_tif285_NeuralNet.html">
     9.5. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Machine_learning/Forssen_tif285_demo-NeuralNet.html">
     9.6. Neural network classifier demonstration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Machine_learning/Bayesian_neural_networks_tif285.html">
     9.7. Bayesian neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Machine_learning/lecture_24.html">
     9.8. Lecture 24
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Machine_learning/demo-Bayesian_neural_networks_tif285.html">
     9.9. Variational Inference: Bayesian Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Machine_learning/Convolutional_neural_network_explained.html">
     9.10. What is a convolutional neural network?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../SVD/svd.html">
   10. PCA, SVD, and all that
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../SVD/lecture_25.html">
     10.1. Lecture 25
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/SVD/linear_algebra_games_including_SVD.html">
     10.2. Linear algebra games including SVD for PCA
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Mini-projects
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/mini-projects/mini-project_I_toy_model_of_EFT.html">
   Mini-project I: Parameter estimation for a toy model of an EFT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/mini-projects/model-selection_mini-project-IIa.html">
   Mini-project IIa: Model selection basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/mini-projects/model-selection_mini-project-IIb_How_many_lines_ptemcee.html">
   Mini-project IIb: How many lines are there?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/mini-projects/mini-project_IIIa_bayesian_optimization.html">
   Mini-project IIIa: Bayesian optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/mini-projects/mini-project_IIIb_Bayesian_neural_networks_from_demo.html">
   Mini-project IIIb: Bayesian Neural Networks
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reference material
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../zbibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../related_topics.html">
   Related topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Reference/installing_anaconda.html">
   Using Anaconda
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Reference/using_github.html">
   Using GitHub
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Reference/python_jupyter.html">
   Python and Jupyter notebooks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Reference/Jupyter_Python_intro_01.html">
     Python and Jupyter notebooks: part 01
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebooks/Reference/Jupyter_Python_intro_02.html">
     Python and Jupyter notebooks: part 02
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../jb_tests.html">
   Examples: Jupyter jb-book
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Notebook keys
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/Basics/simple_sum_product_rule_KEY.html">
   Checking the sum and product rules, and their consequences
   <span style="color: red">
    Key
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/Basics/medical_example_by_Bayes_KEY.html">
   Standard medical example by applying Bayesian rules of probability
   <span style="color: red">
    Key
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks/Basics/radioactive_lighthouse_exercise_key.html">
   Radioactive lighthouse problem
   <span style="color: red">
    Key
   </span>
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/Maximum_entropy/lecture_21.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/furnstahl/Physics-8820"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/furnstahl/Physics-8820/issues/new?title=Issue%20on%20page%20%2Fcontent/Maximum_entropy/lecture_21.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#follow-up-to-gaussian-process-exercises">
   Follow-up to Gaussian process exercises
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-entropy">
   Maximum entropy
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principle-of-maximum-entropy">
     Principle of Maximum Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-1-gaussian-distribution">
     Example 1: Gaussian distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-2-poisson-distributions">
     Example 2: Poisson distributions
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="lecture-21">
<h1><span class="section-number">8.1. </span>Lecture 21<a class="headerlink" href="#lecture-21" title="Permalink to this headline">¶</a></h1>
<div class="section" id="follow-up-to-gaussian-process-exercises">
<h2>Follow-up to Gaussian process exercises<a class="headerlink" href="#follow-up-to-gaussian-process-exercises" title="Permalink to this headline">¶</a></h2>
<p>Return to the notebook <a class="reference internal" href="../../notebooks/Gaussian_processes/Gaussian_processes_exercises.html"><span class="doc std std-doc">Exercise: Gaussian Process models with GPy</span></a>.</p>
<ul class="simple">
<li><p>Do sampling of different covariant functions in <em>2 Sampling from a Gaussian Process</em>.</p>
<ul>
<li><p>Predict <code class="docutils literal notranslate"><span class="pre">nsamples</span> <span class="pre">=</span> <span class="pre">50</span></code>.</p></li>
</ul>
</li>
<li><p>Try some combinations</p>
<ul>
<li><p>linear <span class="math notranslate nohighlight">\(\longrightarrow\)</span> polynomial (try two to get quadratic). [See kernels.pdf Fig 1.1 (on page 2) and Fig. 1.2 (on page 4).]</p></li>
</ul>
</li>
<li><p>For Gaussian Process Regression Model</p>
<ul>
<li><p>distinguish between noise in <em>data</em> and noise in <em>model</em></p></li>
<li><p>compare the true function in red to the curves</p></li>
</ul>
</li>
<li><p>Other things you might play with:</p>
<ul>
<li><p>Add a function for the true result (no noise) and add it (in red) to the plots.</p></li>
<li><p>Compare small data noise vs. large data noise.</p></li>
<li><p>Try making lengthscale very small <span class="math notranslate nohighlight">\(\Lra\)</span> explain the result. [Should return to the mean after a few length scales.]</p></li>
<li><p>Try optimizing with lengthscale very small (it doesn’t change <span class="math notranslate nohighlight">\(\Lra\)</span> optimize fails).</p></li>
<li><p>With a good optimization, explore how well red line is withing the 95% bands.</p>
<ul>
<li><p>relation to prior (mean zero, input variance)</p></li>
<li><p>What if I extend the range of <code class="docutils literal notranslate"><span class="pre">Xtrue</span></code>?</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="maximum-entropy">
<h2>Maximum entropy<a class="headerlink" href="#maximum-entropy" title="Permalink to this headline">¶</a></h2>
<p>A good reference here is Chapter 5 in Sivia <span id="id1">[<a class="reference internal" href="../zbibliography.html#id9">SS06</a>]</span>.
The plan is to step through <a class="reference internal" href="../../notebooks/Maximum_entropy/MaxEnt.html"><span class="doc std std-doc">Ignorance pdfs: Indifference and translation groups</span></a>, then <a class="reference internal" href="../../notebooks/Maximum_entropy/Pdfs_from_MaxEnt.html"><span class="doc std std-doc">MaxEnt for deriving some probability distributions</span></a> as a class exercise. As time permits, we’ll do <a class="reference internal" href="../../notebooks/Maximum_entropy/MaxEnt_Function_Reconstruction.html"><span class="doc std std-doc">Maximum Entropy for reconstructing a function from its moments</span></a>.</p>
<p>Notes on <a class="reference internal" href="../../notebooks/Maximum_entropy/MaxEnt.html"><span class="doc std std-doc">Ignorance pdfs: Indifference and translation groups</span></a>:</p>
<ul>
<li><p>Ignorance pdfs <span class="math notranslate nohighlight">\(\longrightarrow\)</span> when we don’t have constraints or extra knowledge that breaks symmetries.</p>
<ol>
<li><p>permutation symmetry <span class="math notranslate nohighlight">\(\longrightarrow\)</span> die <span class="math notranslate nohighlight">\(\Lra\)</span>
1/(# choices) [discrete]</p></li>
<li><p>translational invariance <span class="math notranslate nohighlight">\(\longrightarrow\)</span> <span class="math notranslate nohighlight">\(p(x|I) = \text{constant}\)</span> (in allowed region)</p></li>
<li><p>scale invariance <span class="math notranslate nohighlight">\(\longrightarrow\)</span> <span class="math notranslate nohighlight">\(p(x|I) \propto 1/x\)</span></p>
<ul>
<li><p>How to derive?</p></li>
<li><p>First check that it works: <span class="math notranslate nohighlight">\(p(x|I) = \lambda p(\lambda x|I)\)</span> <span class="math notranslate nohighlight">\(\Lra\)</span> <span class="math notranslate nohighlight">\(\frac{c}{x} = \frac{\lambda c}{\lambda x} = \frac{c}{x}\)</span>. Check!</p></li>
<li><p>Now more general proof: assume <span class="math notranslate nohighlight">\(p(x|I) \propto x^{\alpha}\)</span>
<span class="math notranslate nohighlight">\(\Lra\)</span> <span class="math notranslate nohighlight">\(x^\alpha = \lambda (\lambda x)^{\alpha}
= \lambda^{1+\alpha} x^\alpha\)</span> <span class="math notranslate nohighlight">\(\Lra\)</span> <span class="math notranslate nohighlight">\(\alpha = -1\)</span>. Check!</p></li>
<li><p>Still more general: set <span class="math notranslate nohighlight">\(\lambda = 1 + \epsilon\)</span> with <span class="math notranslate nohighlight">\(\epsilon \ll 1\)</span>, and solve to <span class="math notranslate nohighlight">\(\mathcal{O}(\epsilon)\)</span>: <span class="math notranslate nohighlight">\(p(x) = (1+\epsilon)(p(x)+\epsilon\frac{dp}{dx})\)</span> <span class="math notranslate nohighlight">\(\Lra\)</span> <span class="math notranslate nohighlight">\(p(x) + x \frac{dp}{dx} = 0\)</span></p>
<div class="math notranslate nohighlight">
\[
             \Lra \int_{p(x_0)}^{p(x)} \frac{dp}{p}
             = \int_{x_0}^x \frac{dx'}{x'}
             \ \Lra\ 
             \log\frac{p(x)}{p(x_0)} = \log\frac{x_0}{x}
             \ \Lra\  p(x) = \left(\frac{p(x_0)}{x_0}\right)\frac{1}{x}
            \]</div>
<p>so <span class="math notranslate nohighlight">\(p(x) \propto 1/x\)</span>.</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p>Step quickly through Symmetry invariance.</p>
<ul class="simple">
<li><p>Basically using a change of variables for the symmetry, which means a Jacobian.</p></li>
<li><p>For the linear model: <span class="math notranslate nohighlight">\(y_{\rm th}(x) = \theta_1 x + \theta_0\)</span>, which we could write the other way around: <span class="math notranslate nohighlight">\(x_{\rm th}(y) = \theta'_1 y + \theta'_0\)</span>, and these probabilities (not densities!) should be equal:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
     p(\theta_0,\theta_1|I) d\theta_0 d\theta_1
       = p(\theta'_0,\theta'_1|I) d\theta'_0 d\theta'_1
    \]</div>
<ul class="simple">
<li><p>We can relate the primed and unprimed <span class="math notranslate nohighlight">\(\theta\)</span>’s by substituting:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    y = \theta_1 x + \theta_0 = \theta_1(\theta'_1 y + \theta'_0) + \theta_0
    = \theta_1 \theta'_1 y + \theta_1\theta'_0 + \theta_0
    \]</div>
<div class="math notranslate nohighlight">
\[
   \Lra \theta_1 \theta'_1 =1, \quad \theta_1\theta'_0+\theta_0 = 0
   \quad\Lra\quad \theta_1' = \theta_1^{-1}, \quad \theta'_0 = -\theta_1^{-1}\theta_0
    \]</div>
<ul class="simple">
<li><p>This lets us calculate the Jacobian:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
     \left| \det\pmatrix{ 
     \frac{\partial\theta_0}{\partial\theta'_{0}} &amp;
     \frac{\partial\theta_0}{\partial\theta'_{1}} \\
     \frac{\partial\theta_1}{\partial\theta'_{0}} &amp;
     \frac{\partial\theta_1}{\partial\theta'_{1}}}
     \right|
     =
     \left| 
     \begin{array}{cc}
       -\theta'_1{}^{-1} &amp; \theta'_0 \\ 
     0 &amp; -\theta'_1{}^{-2}
     \end{array} 
     \right|
     = \frac{1}{\theta'_1{}^3} = \theta_1^3 .
    \end{split}\]</div>
<ul class="simple">
<li><p>That means that</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
     p(\theta_0,\theta_1|I) d\theta_0 d\theta_1
     &amp; = p(-\theta_1^{-1}\theta_0,\theta_1^{-1}|I)  d\theta_0 d\theta_1 \frac{1}{\theta_1^3} \\
     \mbox{or}\ \theta_1^3 p(\theta_0,\theta_1|I)  &amp;= p(-\theta_1^{-1}\theta_0,\theta_1^{-1}|I).
    \end{align}\end{split}\]</div>
<ul class="simple">
<li><p>One possible solution is</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
      p(\theta_0,\theta_1|I) \propto (1+\theta_1^2)^{-3/2} .
    \]</div>
</li>
</ul>
<div class="section" id="principle-of-maximum-entropy">
<h3>Principle of Maximum Entropy<a class="headerlink" href="#principle-of-maximum-entropy" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Arguing from monkeys distributing <span class="math notranslate nohighlight">\(N\)</span> balls into <span class="math notranslate nohighlight">\(M\)</span> boxes, so <span class="math notranslate nohighlight">\(n_i\)</span> in each box and <span class="math notranslate nohighlight">\(N = \sum_{i=1}^M n_i\)</span>.</p></li>
<li><p>We’ll let them do this many times, subject to the constraints described by <span class="math notranslate nohighlight">\(I\)</span>.</p>
<ul class="simple">
<li><p>The idea is to find the pdf specified by <span class="math notranslate nohighlight">\(p_i = n_i/N\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> that appears most often <span class="math notranslate nohighlight">\(\Lra\)</span> this best represents our state of knowledge.</p></li>
</ul>
</li>
<li><p>So this becomes a matter of counting microstates (i.e., a particular distribution <span class="math notranslate nohighlight">\(\{n_i\}\)</span>) that are most likely given the constraints.</p>
<ul>
<li><p>We’ll let <span class="math notranslate nohighlight">\(F(\{p_i\}) = \text{# of ways to get } \{n_i\} / \text{total # of ways} = M^N\)</span>.</p></li>
<li><p>Now do some combinatorics <span class="math notranslate nohighlight">\(\Lra\)</span> this is a multinomial distribution (we use Stirling here: <span class="math notranslate nohighlight">\(n! \approx n\log n - n\)</span>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
          \log F(\{p_i\}) &amp;= \log(N!) - \sum_{i=1}^M \log(n_i!) -     N\log M \\
          &amp; \approx -N\log M + N\log N - \sum_{i=1}^M n_i\log(n_i) \\
          &amp; \approx -N\log M - N \sum_{i=1}^M p_i\log(p_i)
        \end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(p_i = n_i/N\)</span>.</p>
</li>
</ul>
</li>
<li><p>This tells us that the key piece to maximize is the entropy:</p>
<div class="math notranslate nohighlight">
\[ S = - \sum_{i=1}^M p_i \log(p_i) . \]</div>
</li>
<li><p>There are several arguments for maximizing the entropy:</p>
<ol class="simple">
<li><p>information theory, which says says maximum entropy equals minimum information (Shannon, 1948);</p></li>
<li><p>logical consistency (Shore and Johnson, 1960);</p></li>
<li><p>uncorrelated assignments are related monotonically to <span class="math notranslate nohighlight">\(S\)</span> (Skilling, 1988).</p></li>
</ol>
</li>
<li><p>The third one tells us that unless you know specifically about correlations, it should not be in your probability assignment. One finds that entropy maximization satisfies this condition (see the notebook for a comparison of different possibilities for a test problem).</p></li>
<li><p>The continuous version of entropy is</p>
<div class="math notranslate nohighlight">
\[
       S[p(x)] = - \int dx p(x) \log\Bigl(\frac{p(x)}{m(x)}\Bigr)
    \]</div>
<p>where <span class="math notranslate nohighlight">\(m(x)\)</span> is a measure function. It is there to ensure that <span class="math notranslate nohighlight">\(S[p]\)</span> is invariant under <span class="math notranslate nohighlight">\(x\rightarrow y=f(x)\)</span>. Typically this means <span class="math notranslate nohighlight">\(m(x) = \)</span> constant.</p>
</li>
</ul>
</div>
<div class="section" id="example-1-gaussian-distribution">
<h3>Example 1: Gaussian distribution<a class="headerlink" href="#example-1-gaussian-distribution" title="Permalink to this headline">¶</a></h3>
<p>The constraints are normalization and a known variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
   \int_{0}^\infty dx\, p(x) &amp;= 1, \quad x\geq 0 \\
   \int_{0}^\infty dx\, (x-\mu)^2 p(x) &amp;= \sigma^2
\end{align}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\Lra\)</span> maximize</p>
<div class="math notranslate nohighlight">
\[
  Q(p;\lambda_0,\lambda_1) = - \int dx\, p(x)\log\Bigl(\frac{p(x)}{m(x)}\Bigr) + \lambda_0 \bigl(1 - \int dx\, p(x)\bigr)
  + \lambda_1 \bigl(\sigma^2 - \int dx\, (x-\mu)^2 p(x)\bigr) ,
\]</div>
<p>with uniform <span class="math notranslate nohighlight">\(m(x)\)</span> (we take <span class="math notranslate nohighlight">\(m(x) = 1\)</span> below). The maximization is straightforward.</p>
<p><strong>Step 1:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \frac{\delta Q}{\delta p(x)}
 &amp;= -\log\frac{p(x)}{1} - \frac{p(x)}{p(x)} - \lambda_0 - \lambda_1 (x-\mu)^2 \\
  \frac{\delta Q}{\delta \lambda_0} &amp;= 1 - \int_{-\infty}^{\infty} dx\, p(x) \quad\text{and}\quad
 \frac{\delta Q}{\delta \lambda_1} = \sigma^2 - \int_{-\infty}^{\infty} dx\, (x-\mu)^2 p(x) .
\end{align}\end{split}\]</div>
<p><strong>Step 2:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
 &amp; \frac{\delta Q}{\delta p(x)} = 0 \Lra \log p(x) = -(1 + \lambda_0) - \lambda_1 (x-\mu)^2 \\
 &amp; \Lra p(x) = e^{-1+\lambda_0}e^{-\lambda_1 (x-\mu)^2} .
\end{align}\end{split}\]</div>
<p><strong>Step 3:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
 &amp; \frac{\partial Q}{\partial \lambda_0} = 0 \Lra \int_{-\infty}^{\infty} dx\, e^{-1+\lambda_0}e^{-\lambda_1 (x-\mu)^2} =
 e^{-1+\lambda_0} \sqrt{\frac{\pi}{\lambda_1}} = 1
 \Lra e^{-1+\lambda_0} = \sqrt{\frac{\lambda_1}{\pi}}   \\
 &amp; \frac{\partial Q}{\partial \lambda_1} = 0 \Lra \int_{-\infty}^{\infty} dx\, (x-\mu)^2 e^{-1+\lambda_0}e^{-\lambda_1 (x-\mu)^2} = \underbrace{e^{-1+\lambda_0}\frac{1}{\lambda_1^{3/2}}}_{1/\sqrt{\pi}\lambda_1} \underbrace{\int_{-\infty}^{\infty} dy\, y^2 e^{-y^2}}_{\sqrt{\pi}/2} = \sigma^2
 \Lra \lambda_1 = \frac{1}{2\sigma^2}
\end{align}\end{split}\]</div>
<p>Putting it together we get our good friend the Gaussian distribution:</p>
<div class="math notranslate nohighlight">
\[
  p(x) = \frac{1}{\sqrt{2\pi\sigma^2}}  e^{-(x-\mu)^2/2\sigma^2} .
\]</div>
</div>
<div class="section" id="example-2-poisson-distributions">
<h3>Example 2: Poisson distributions<a class="headerlink" href="#example-2-poisson-distributions" title="Permalink to this headline">¶</a></h3>
<p>The constraints are normalization and a known mean:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
   \int_{0}^\infty dx\, p(x) &amp;= 1, \quad x\geq 0 \\
   \int_{0}^\infty dx\, x p(x) &amp;= \mu
\end{align}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\Lra\)</span> maximize</p>
<div class="math notranslate nohighlight">
\[
  Q(p;\lambda_0,\lambda_1) = - \int dx\, p(x)\log\Bigl(\frac{p(x)}{m(x)}\Bigr) + \lambda_0 \bigl(1 - \int dx\, p(x)\bigr)
  + \lambda_1 \bigl(\mu - \int dx\, x p(x)\bigr) ,
\]</div>
<p>with uniform <span class="math notranslate nohighlight">\(m(x)\)</span>. The maximization is again straightforward:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
 &amp; \frac{\delta Q}{\delta p(x)}
 = -\log\frac{p(x)}{1} - \frac{p(x)}{p(x)} - \lambda_0 - \lambda_1 x = 0 \\
 &amp; \Lra \log p(x) = -(1 + \lambda_0) - \lambda_1 x \\
 &amp; \Lra p(x) = e^{-1+\lambda_0}e^{-\lambda_1 x}
\end{align}\end{split}\]</div>
<p>Finally, we determine <span class="math notranslate nohighlight">\(\lambda_0\)</span> and <span class="math notranslate nohighlight">\(\lambda_1\)</span> from the constraints:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  e^{1+\lambda_0}\int_0^\infty dx\, e^{-\lambda_1 x}
    = e^{-(1+\lambda_0)}\frac{1}{\lambda_1} = 1
    \quad&amp;\Lra\quad \lambda_1 = e^{-(1+\lambda_0)} \\
   \int_0^\infty dx\, \underbrace{e^{-(1+\lambda_0)}}_{\lambda_1}
 \underbrace{e^{-\lambda_1 x}x}_{1/\lambda_1^2}
  = \mu
  \quad&amp;\Lra\quad \lambda_1 = \frac{1}{\mu} .
\end{align}\end{split}\]</div>
<p>Substituting we get the Poisson distribution:</p>
<div class="math notranslate nohighlight">
\[
  p(x) = \frac{1}{\mu}  e^{-x/\mu} .
\]</div>
<p>Try the other examples!</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "furnstahl/Physics-8820",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/Maximum_entropy"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="max_ent.html" title="previous page"><span class="section-number">8. </span>Maximum entropy</a>
    <a class='right-next' id="next-link" href="../../notebooks/Maximum_entropy/MaxEnt.html" title="next page"><span class="section-number">8.2. </span>Ignorance pdfs: Indifference and translation groups</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dick Furnstahl<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>