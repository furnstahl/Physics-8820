
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>9.4. What Are Neural Networks? &#8212; Learning from data</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"N": ["\\mathbb{N}"], "Z": ["\\mathbb{Z}"], "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"], "alphavec": ["\\boldsymbol{\\alpha}"], "muvec": ["\\boldsymbol{\\mu}"], "phivec": ["\\boldsymbol{\\phi}"], "sigmavec": ["\\boldsymbol{\\sigma}"], "Sigmavec": ["\\boldsymbol{\\Sigma}"], "thetavec": ["\\boldsymbol{\\theta}"], "thetavechat": ["\\widehat\\thetavec"], "avec": ["\\boldsymbol{a}"], "Bvec": ["\\boldsymbol{B}"], "fvec": ["\\boldsymbol{f}"], "mvec": ["\\boldsymbol{m}"], "qvec": ["\\boldsymbol{q}"], "rvec": ["\\boldsymbol{r}"], "uvec": ["\\boldsymbol{u}"], "wvec": ["\\boldsymbol{w}"], "xvec": ["\\boldsymbol{x}"], "yvec": ["\\boldsymbol{y}"], "Lra": ["\\Longrightarrow"], "abar": ["\\overline a"], "Xbar": ["\\overline X"], "alphahat": ["\\widehat\\alpha"], "Hhat": ["\\hat H"], "yth": ["y_{\\text{th}}"], "yexp": ["y_{\\text{exp}}"], "ym": ["y_{\\text{m}}"], "gs": ["{0}"]}}})</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="9.5. Neural networks" href="Forssen_tif285_NeuralNet.html" />
    <link rel="prev" title="9.3. Lecture 23" href="../../content/Machine_learning/lecture_23.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/8820_icon.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning from data</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../about.html">
   About this Jupyter Book
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Course overview
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/Course/overview.html">
   Objectives
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Topics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Basics/basics.html">
   1. Basics of Bayesian statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Basics/lecture_01.html">
     1.1. Lecture 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/Exploring_pdfs.html">
     1.2. Exploring PDFs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/simple_sum_product_rule.html">
     1.3. Checking the sum and product rules, and their consequences
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Basics/lecture_02.html">
     1.4. Lecture 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/Bayesian_updating_coinflip_interactive.html">
     1.5. Interactive Bayesian updating: coin flipping example
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/medical_example_by_Bayes.html">
     1.6. Standard medical example by applying Bayesian rules of probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/radioactive_lighthouse_exercise.html">
     1.7. Radioactive lighthouse problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Basics/lecture_03.html">
     1.8. Lecture 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Parameter_estimation/param_est.html">
   2. Bayesian parameter estimation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Parameter_estimation/lecture_04.html">
     2.1. Lecture 4: Parameter estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/parameter_estimation_Gaussian_noise.html">
     2.2. Parameter estimation example: Gaussian noise and averages
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/Assignment_extending_radioactive_lighthouse.html">
     2.3. Assignment: 2D radioactive lighthouse location using MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Parameter_estimation/lecture_05.html">
     2.4. Lecture 5
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/parameter_estimation_fitting_straight_line_I.html">
     2.5. Parameter estimation example: fitting a straight line
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/demo-ModelValidation.html">
     2.6. Linear Regression and Model Validation demonstration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Parameter_estimation/lecture_06.html">
     2.7. Lecture 6
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/amplitude_in_presence_of_background.html">
     2.8. Amplitude of a signal in the presence of background
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/Assignment_parameter_estimation_followups.html">
     2.9. Assignment: Follow-ups to Parameter Estimation notebooks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/exercise_LinearRegression.html">
     2.10. Linear Regression exercise
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/linear_algebra_games_I.html">
     2.11. Linear algebra games including SVD for PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/fluctuation_trend_with_number_of_points_and_data_errors.html">
     2.12. Follow-up: fluctuation trends with # of points and data errors
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/MCMC_sampling_I/MCMC_sampling_I.html">
   3. MCMC sampling I
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/MCMC_sampling_I/lecture_07.html">
     3.1. Lecture 7
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_I/Metropolis_Poisson_example.html">
     3.2. Metropolis-Hasting MCMC sampling of a Poisson distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/MCMC_sampling_I/lecture_08.html">
     3.3. Lecture 8
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_I/MCMC-random-walk-and-sampling.html">
     3.4. Exercise: Random walk
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Why_Bayes_is_better/bayes_is_better.html">
   4. Why Bayes is better
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Why_Bayes_is_better/lecture_09.html">
     4.1. Lecture 9
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/bayes_billiard.html">
     4.2. A Bayesian Billiard game
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Why_Bayes_is_better/lecture_10.html">
     4.3. Lecture 10
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/parameter_estimation_fitting_straight_line_II.html">
     4.4. Parameter estimation example: fitting a straight line II
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Why_Bayes_is_better/lecture_11.html">
     4.5. Lecture 11
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/error_propagation_to_functions_of_uncertain_parameters.html">
     4.6. Error propagation: Example 3.6.2 in Sivia
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/visualization_of_CLT.html">
     4.7. Visualization of the Central Limit Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/correlation_intuition.html">
     4.8. Building intuition about correlations (and a bit of Python linear algebra)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Why_Bayes_is_better/lecture_12.html">
     4.9. Lecture 12
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_I/MCMC-diagnostics.html">
     4.10. Overview: MCMC Diagnostics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Why_Bayes_is_better/lecture_13.html">
     4.12. Lecture 13
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/dealing_with_outliers.html">
     4.13. Dealing with outliers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Model_selection/model_selection.html">
   5. Model selection
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Model_selection/lecture_14.html">
     5.1. Lecture 14
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Model_selection/lecture_15.html">
     5.2. Lecture 15
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Model_selection/Evidence_for_model_EFT_coefficients.html">
     5.3. Evidence calculation for EFT expansions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Model_selection/lecture_16.html">
     5.4. Lecture 16
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../mini-projects/MCMC-parallel-tempering_ptemcee.html">
     5.5. Example: Parallel tempering for multimodal distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../mini-projects/MCMC-parallel-tempering_ptemcee_vs_zeus.html">
     5.6. Example: Parallel tempering for multimodal distributions vs. zeus
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/MCMC_sampling_II/MCMC_sampling_II.html">
   6. MCMC sampling II
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/MCMC_sampling_II/lecture_17.html">
     6.1. Lecture 17
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/chi_squared_tests.html">
     6.2. Quick check of the distribution of normal variables squared
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/Liouville_theorem_visualization.html">
     6.3. Liouville Theorem Visualization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/Orbital_eqs_with_different_algorithms.html">
     6.4. Solving orbital equations with different algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/MCMC_sampling_II/lecture_18.html">
     6.5. Lecture 18
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/PyMC3_intro_updated.html">
     6.6. PyMC3 Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/PyMC3_docs_getting_started_updated.html">
     6.7. Getting started with PyMC3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/parameter_estimation_Gaussian_noise_compare_samplers.html">
     6.8. Comparing samplers for a simple problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../mini-projects/zeus_multimodal.html">
     6.9. zeus: Sampling from multimodal distributions
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Gaussian_processes/gaussian_processes.html">
   7. Gaussian processes
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Gaussian_processes/lecture_19.html">
     7.1. Lecture 19
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Gaussian_processes/demo-GaussianProcesses.html">
     7.2. Gaussian processes demonstration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Gaussian_processes/GaussianProcesses.html">
     7.3. Learning from data: Gaussian processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Gaussian_processes/Gaussian_processes_exercises.html">
     7.4. Exercise: Gaussian Process models with GPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Gaussian_processes/lecture_20.html">
     7.5. Lecture 20
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Maximum_entropy/max_ent.html">
   8. Assigning probabilities
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Maximum_entropy/lecture_21.html">
     8.1. Lecture 21
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Maximum_entropy/MaxEnt.html">
     8.2. Ignorance pdfs: Indifference and translation groups
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Maximum_entropy/Pdfs_from_MaxEnt.html">
     8.3. MaxEnt for deriving some probability distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Maximum_entropy/MaxEnt_Function_Reconstruction.html">
     8.4. Maximum Entropy for reconstructing a function from its moments
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Maximum_entropy/demo-MaxEnt.html">
     8.5. Making figures for Ignorance PDF notebook
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../../content/Machine_learning/machine_learning.html">
   9. Machine learning: Bayesian methods
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Machine_learning/lecture_22.html">
     9.1. Lecture 22
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bayesian_optimization.html">
     9.2. Bayesian Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Machine_learning/lecture_23.html">
     9.3. Lecture 23
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     9.4. What Are Neural Networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Forssen_tif285_NeuralNet.html">
     9.5. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Forssen_tif285_demo-NeuralNet.html">
     9.6. Neural network classifier demonstration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bayesian_neural_networks_tif285.html">
     9.7. Bayesian neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Machine_learning/lecture_24.html">
     9.8. Lecture 24
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="demo-Bayesian_neural_networks_tif285.html">
     9.9. Variational Inference: Bayesian Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Convolutional_neural_network_explained.html">
     9.10. What is a convolutional neural network?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/SVD/svd.html">
   10. PCA, SVD, and all that
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../SVD/linear_algebra_games_including_SVD.html">
     10.1. Linear algebra games including SVD for PCA
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Mini-projects
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../mini-projects/mini-project_I_toy_model_of_EFT.html">
   Mini-project I: Parameter estimation for a toy model of an EFT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mini-projects/model-selection_mini-project-IIa.html">
   Mini-project IIa: Model selection basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mini-projects/model-selection_mini-project-IIb_How_many_lines_ptemcee.html">
   Mini-project IIb: How many lines are there?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mini-projects/mini-project_IIIa_bayesian_optimization.html">
   Mini-project IIIa: Bayesian optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mini-projects/mini-project_IIIb_Bayesian_neural_networks_from_demo.html">
   Mini-project IIIb: Bayesian Neural Networks
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reference material
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/zbibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/related_topics.html">
   Related topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/Reference/installing_anaconda.html">
   Using Anaconda
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/Reference/using_github.html">
   Using GitHub
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Reference/python_jupyter.html">
   Python and Jupyter notebooks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Reference/Jupyter_Python_intro_01.html">
     Python and Jupyter notebooks: part 01
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Reference/Jupyter_Python_intro_02.html">
     Python and Jupyter notebooks: part 02
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/jb_tests.html">
   Examples: Jupyter jb-book
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Notebook keys
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Basics/simple_sum_product_rule_KEY.html">
   Checking the sum and product rules, and their consequences
   <span style="color: red">
    Key
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Basics/medical_example_by_Bayes_KEY.html">
   Standard medical example by applying Bayesian rules of probability
   <span style="color: red">
    Key
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Basics/radioactive_lighthouse_exercise_key.html">
   Radioactive lighthouse problem
   <span style="color: red">
    Key
   </span>
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/notebooks/Machine_learning/Neural_networks_explained.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/furnstahl/Physics-8820"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/furnstahl/Physics-8820/issues/new?title=Issue%20on%20page%20%2Fnotebooks/Machine_learning/Neural_networks_explained.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/furnstahl/Physics-8820/main?urlpath=tree/./LectureNotes/notebooks/Machine_learning/Neural_networks_explained.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#different-types-of-neural-networks">
   Different types of neural networks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-do-neural-networks-actually-work">
   How do neural networks actually work?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-is-the-error-calculated">
   How is the error calculated?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#purpose-of-an-activation-function">
   Purpose of an activation function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-an-optimization-algorithm">
   What is an optimization algorithm?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-physics-example-phase-shifts">
   A physics example: phase shifts
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-input-one-output">
     One input, one output
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-inputs-outputs-hidden-layers">
     Multiple inputs/outputs/hidden layers
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="what-are-neural-networks">
<h1><span class="section-number">9.4. </span>What Are Neural Networks?<a class="headerlink" href="#what-are-neural-networks" title="Permalink to this headline">¶</a></h1>
<p><strong>Notes by Alberto Garcia (2021)</strong></p>
<p>A neural network (NN) is a machine learning technique <strong>that follows a set of rules with the purpose of recognizing patterns. Once the neural network learns these patterns, it can be used to make predictions</strong>. The idea of a neural network came from studying how a brain interprets and predicts outcomes. This led to the idea of writing an algorithm that mimics the brain, which is how neural networks were developed.</p>
<p>The neural networks used in machine learning are called artificial neural networks, as opposed to real neural networks contained in the brain. They are made up of three different components: <strong>an input layer, hidden layer(s), and an output layer</strong>.</p>
<a class="reference internal image-reference" href="../../_images/ANN_example.png"><img alt="../../_images/ANN_example.png" class="align-center" src="../../_images/ANN_example.png" style="width: 600px;" /></a>
<p>Taken from <a class="reference external" href="https://www.researchgate.net/figure/Artificial-neural-network-architecture-ANN-i-h-1-h-2-h-n-o_fig1_321259051">https://www.researchgate.net/figure/Artificial-neural-network-architecture-ANN-i-h-1-h-2-h-n-o_fig1_321259051</a>.</p>
<p>The input layer is used to pass in whatever we are inputting into the neural network. The hidden layers are responsible for learning how to map the input to the output. The output layer is responsible for giving us the outputs of the neural network given our inputs.</p>
<p>A neural network can learn three different ways: supervised, unsupervised, and reinforced. I will only comment on the first two. In supervised learning, the desired output is already known and the neural network will modify itself until the it obtains the desired result. In unsupervised learning, the desired output is not known. The neural network will be trained and  tested against known data. A cost function then tells the neural network how far off the target it was. The neural network will then retrain and retest for better accuracy.</p>
<p>More information can be found at: <br></p>
<ol class="simple">
<li><p><a class="reference external" href="https://towardsdatascience.com/understanding-neural-networks-what-how-and-why-18ec703ebd31">https://towardsdatascience.com/understanding-neural-networks-what-how-and-why-18ec703ebd31</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/first-neural-network-for-beginners-explained-with-code-4cfd37e06eaf">https://towardsdatascience.com/first-neural-network-for-beginners-explained-with-code-4cfd37e06eaf</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6">https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6</a></p></li>
</ol>
<div class="section" id="different-types-of-neural-networks">
<h2>Different types of neural networks<a class="headerlink" href="#different-types-of-neural-networks" title="Permalink to this headline">¶</a></h2>
<p>There are many different types of neural networks currently being used by data scientists around the world, but the most common are: perceptron, feed forward, recurrent, and convolutional.</p>
<p>The perceptron is composed of an input and output layer. The only pattern this type of neural network is able to learn is linear patterns. The feed-forward neural network, also known as a multi-layer perceptron, is composed of an input layer, hidden layers, and output layer. It is not limited to learning linear patterns, but data only moves in one direction.</p>
</div>
<div class="section" id="how-do-neural-networks-actually-work">
<h2>How do neural networks actually work?<a class="headerlink" href="#how-do-neural-networks-actually-work" title="Permalink to this headline">¶</a></h2>
<p>The key of neural networks is that the layers transform the inputs using a series of mathematical operations in order to learn how to map the inputs to the outputs while producing good predictions.</p>
<p>Say we have a set of data composed of <span class="math notranslate nohighlight">\(x\)</span>’s and <span class="math notranslate nohighlight">\(y\)</span>’s. This data would be split into what are known as training and testing data sets, with both sets containing <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> values. The training set is used to train the neural network. This is what gets inputted into the model and gets weights assigned to. The testing set is used to test the newly trained neural network. Once the model gets trained using the training set, it will be used to predict the <span class="math notranslate nohighlight">\(y\)</span> values from the testing set by using the corresponding <span class="math notranslate nohighlight">\(x\)</span> testing set values. An error that tells you how well your neural network did is then calculated. You can also add a bias to the neural network. This is an extra input, usually denoted as <span class="math notranslate nohighlight">\(b\)</span> or <span class="math notranslate nohighlight">\(w_0\)</span>, with no weights associated to it. The bias is there to ensure that even when there is a <span class="math notranslate nohighlight">\(0\)</span> input, that neuron will still activate.</p>
<a class="reference internal image-reference" href="../../_images/perceptron_node.png"><img alt="../../_images/perceptron_node.png" class="align-center" src="../../_images/perceptron_node.png" style="width: 600px;" /></a>
<p>Taken from: <a class="reference external" href="https://skymind.ai/wiki/neural-network">https://skymind.ai/wiki/neural-network</a></p>
<p>We are given a vector of inputs <span class="math notranslate nohighlight">\(X = (x_1, x_2, x_3, ..., x_n)\)</span>. These inputs are then assigned weights <span class="math notranslate nohighlight">\(W = (w_1, w_2, w_3, ..., w_n)\)</span>. The weights are randomly initialized using initializers. The inputs and weights are then multiplied and passed into each node of the hidden layer. All the neurons in a hidden layer are connected to each and every neuron in the next layer, therefore, each node will receive all of the available data. The results are summed up using a dot product</p>
<div class="math notranslate nohighlight">
\[
W \cdot X = w_1 x_1 + w_2 x_2 + w_3 x_3 + ... + w_n x_n
\]</div>
<p>and passed into an activation function assigned to the hidden layer used to transform the data. The model then outputs a predicted target value <span class="math notranslate nohighlight">\(Y = (y_1, y_2, y_3, ..., y_n)\)</span>. This process is known as forward-propagation. Now, we calculate the error between the predicted target value and the actual value using a loss function. If the error is too large, we get what’s known as back-propagation. This is the process of adjusting the weights for accuracy by minimizing the error. This is done by taking the derivative of the error with respect to every weight in the neural network. These derivatives are known as gradients. We then subtract this gradient from its corresponding weight in order to reduce the error by moving closer to the minimum loss. This is known as gradient descent, by far the most popular optimizer. This process is repeated until a desired accuracy has been reached. Once this is the case, the calculated <span class="math notranslate nohighlight">\(Y = (y_1, y_2, y_3, ..., y_n)\)</span> are sent to the output layer.</p>
<p>The mapping from input layer to output layer can be summarized by</p>
<div class="math notranslate nohighlight">
\[
Y = f \bigg(W \cdot X  + b \bigg)
\]</div>
<p>where <span class="math notranslate nohighlight">\(b\)</span> is the bias and <span class="math notranslate nohighlight">\(f\)</span> is the activation function.</p>
<p>Care must be taken when calculating the gradients. If the derivative is too small, the network will become increasingly difficult to train. This is seen for deep neural networks when choosing an inappropriate activation function, like the sigmoid, since a large change in the input of the sigmoid function will cause a small change in the output.</p>
</div>
<div class="section" id="how-is-the-error-calculated">
<h2>How is the error calculated?<a class="headerlink" href="#how-is-the-error-calculated" title="Permalink to this headline">¶</a></h2>
<p>When building a model, we always seek to minimize the error as much as possible. This error is calculated by what’s known as a loss function. The loss function can be written many different ways, but the most common is mean-square-error defined as</p>
<div class="math notranslate nohighlight">
\[
\mbox{MSE} = \sum^{N}_{i=1} \frac{ (y^{(i)}_T - y^{(i)}_P)^2 }{N}
\]</div>
<p>where <span class="math notranslate nohighlight">\(y^{(i)}_P\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>th predicted value, <span class="math notranslate nohighlight">\(y^{(i)}_T\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>th true or expected value, and <span class="math notranslate nohighlight">\(N\)</span> is the number of data points. This loss function takes the square of the difference between the predicted and expected values and sums them up.</p>
<p>Another example of a loss function is known as the correlated loss function, defined as</p>
<div class="math notranslate nohighlight">
\[
L = \sum^{N}_{i=1} \sum^{N}_{j=1} W_{i,j} R_i R_j
\]</div>
<p>where <span class="math notranslate nohighlight">\(W\)</span> is a correlation matrix and <span class="math notranslate nohighlight">\(W_{i,j}\)</span> are its elements, and <span class="math notranslate nohighlight">\(R_i\)</span> and <span class="math notranslate nohighlight">\(R_j\)</span> are the residuals of the <span class="math notranslate nohighlight">\(y^{(i)}_T\)</span> and <span class="math notranslate nohighlight">\(y^{(i)}_P\)</span>, written as <span class="math notranslate nohighlight">\(R_i = y^{(i)}_T - y^{(i)}_P\)</span> and <span class="math notranslate nohighlight">\(R_j = y^{(j)}_T - y^{(j)}_P\)</span>. This loss function is used to link the previously predicted value with the next. This is how it gets the name correlated loss function, since it correlates the data points. If the correlation matrix <span class="math notranslate nohighlight">\(W\)</span> is diagonal and it’s elements equal <span class="math notranslate nohighlight">\(1\)</span> (<span class="math notranslate nohighlight">\(W_{i,j} = \delta_{i,j}\)</span>), we get back the mean-square-error function.</p>
<p>We can also add what’s known as regulators. These are used to penalize the loss function to prevent the weights from blowing up. This serves to prevent overfitting, which is when the model learns the given data too well and does not generalize to other data. The two popular types of regulators are known as Ridge Regression (L2 norm) and LASSO (L1 norm). I will quote them in the order listed, but not discuss them.</p>
<div class="math notranslate nohighlight">
\[
\mbox{L(loss)} + \lambda \sum^{n}_{j=1} | w_j | \; \mbox{(LASSO)}, \; \; \; \; \; \mbox{L(loss)} + \lambda \sum^{n}_{j=1} w^2_j \; \mbox{(Ridge)}
\]</div>
</div>
<div class="section" id="purpose-of-an-activation-function">
<h2>Purpose of an activation function<a class="headerlink" href="#purpose-of-an-activation-function" title="Permalink to this headline">¶</a></h2>
<p>The purpose of an activation function is to introduce non-linearity to the neural network. Even if the initial data is uniform and scaled, they can easily blow up once they start getting multiplied by weights and summed up. The activation function forces these values back into a manageable range by rescaling them. It’s important to note that activation functions only reside in neurons from hidden and output layers, not input layers.</p>
<p>Examples of popular activation functions are the hyperbolic tangent, <span class="math notranslate nohighlight">\(\tanh(z)\)</span>, which rescales the data between <span class="math notranslate nohighlight">\([-1,1]\)</span>, and the sigmoid, <span class="math notranslate nohighlight">\(f(z) = \frac{1}{1 + e^{-z}}\)</span>, which rescales the data as a continuous range of values between <span class="math notranslate nohighlight">\([0,1]\)</span>.</p>
</div>
<div class="section" id="what-is-an-optimization-algorithm">
<h2>What is an optimization algorithm?<a class="headerlink" href="#what-is-an-optimization-algorithm" title="Permalink to this headline">¶</a></h2>
<p>An optimization algorithm is an algorithm used to minimize the error (loss) as discussed previously. They involve a variable known as the learning rate. This variable ensures that the weights are not changing too fast by scaling the gradients. The most popular is gradient descent, which we already mentioned. Another one more commonly used is known as adaptive moment estimation (ADAM). This works by adding fractions of the previous gradients to the current ones.</p>
</div>
<div class="section" id="a-physics-example-phase-shifts">
<h2>A physics example: phase shifts<a class="headerlink" href="#a-physics-example-phase-shifts" title="Permalink to this headline">¶</a></h2>
<p>Let’s assume we have a dataset that contains scattering energies and their corresponding wave number <span class="math notranslate nohighlight">\(k\)</span>, phase shifts <span class="math notranslate nohighlight">\(\delta\)</span>, and scattering amplitudes <span class="math notranslate nohighlight">\(\tau\)</span>.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(E\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(k\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\delta\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\tau\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(E_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(k_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\delta_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\tau_1\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(E_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(k_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\delta_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\tau_2\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(E_N\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(k_N\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\delta_N\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\tau_N\)</span></p></td>
</tr>
</tbody>
</table>
<div class="section" id="one-input-one-output">
<h3>One input, one output<a class="headerlink" href="#one-input-one-output" title="Permalink to this headline">¶</a></h3>
<p>Let’s first assume we are trying to predict <span class="math notranslate nohighlight">\(\delta\)</span> with only the energies <span class="math notranslate nohighlight">\(E\)</span>. We can
write these values as vectors:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\vec{E} = (E_1, E_2, ..., E_N)\\
\vec{\delta} = (\delta_1, \delta_2, ..., \delta_N)
\end{split}\]</div>
<p>that compose dataset <span class="math notranslate nohighlight">\(d\)</span>. The first step of the process is to normalize and split the dataset into training and testing sets. There are a few ways of doing it, but the python library sklearn has a function called train_test_split. We pick a percentage of the original dataset <span class="math notranslate nohighlight">\(d\)</span> to be the testing set and we shuffle the dataset before splitting to ensure that the model is as generalized as possible. We shall call the training vectors</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\vec{E}_{\mbox{train}} = (E_1, E_2, ..., E_i)\\
\vec{\delta}_{\mbox{train}} = (\delta_1, \delta_2, ..., \delta_i)
\end{split}\]</div>
<p>and they now go up to index <span class="math notranslate nohighlight">\(i\)</span> where <span class="math notranslate nohighlight">\(i &lt; N\)</span>. The testing set will be</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\vec{E}_{\mbox{test}} = (E_1, E_2, ..., E_j)\\
\vec{\delta}_{\mbox{test}} = (\delta_1, \delta_2, ..., \delta_j)
\end{split}\]</div>
<p>where the elements go up to index <span class="math notranslate nohighlight">\(j\)</span> where <span class="math notranslate nohighlight">\(j &lt; i\)</span>. A good split is usually <span class="math notranslate nohighlight">\(20\%\)</span> testing to <span class="math notranslate nohighlight">\(80\%\)</span> training.</p>
<p>We will no longer touch the testing dataset. Our focus will be solely on the training set. Assuming we have a <span class="math notranslate nohighlight">\(2\)</span>-layer neural network, one hidden layer with <span class="math notranslate nohighlight">\(k\)</span> neurons (<span class="math notranslate nohighlight">\(i &gt; k &gt; j\)</span>), the neural network will look like:</p>
<a class="reference internal image-reference" href="../../_images/NN_EnergyPhase.png"><img alt="../../_images/NN_EnergyPhase.png" class="align-center" src="../../_images/NN_EnergyPhase.png" style="width: 500px;" /></a>
<p>Each line (known as synapse) connecting the input layer to the hidden layer corresponds to <strong>ALL</strong> the <span class="math notranslate nohighlight">\(\vec{E}_{\mbox{train}}\)</span> dataset (all the values of energy) getting passed into <strong>EACH</strong> node in the hidden layer. The hidden layer output is the predicted phase shifts. Each line connecting the input layer to the hidden layer is a weight, so each line going to the first neuron in the hidden layer represents <span class="math notranslate nohighlight">\(w_{1}\)</span>, <span class="math notranslate nohighlight">\(w_{2}\)</span>, <span class="math notranslate nohighlight">\(\cdots\)</span> <span class="math notranslate nohighlight">\(w_{i}\)</span>. The rest of the weights follow suite. <strong>The weights for a specific energy for different hidden layer nodes in general are not the same</strong>. So even though we have one kind of input (call it a class), each point gets assigned a weight and is passed into each node of the hidden layer.</p>
<p>Let us look at one hidden layer neuron for simplicity:</p>
<a class="reference internal image-reference" href="../../_images/one_neuron.png"><img alt="../../_images/one_neuron.png" class="align-center" src="../../_images/one_neuron.png" style="width: 500px;" /></a>
<p>The synapses connecting the hidden layer to the output are also weights obtained from the activation function. These weights tell you how the output should look according to their values. Once a predicted value for the phase shift is outputted, it is compared to the true value from the <span class="math notranslate nohighlight">\(\vec{\delta}_{\mbox{train}}\)</span> dataset. If the error given by the loss function is too large, we undergo back-propagation. The weights get adjusted by an optimization algorithm such as gradient descent, which is the process of adjusting the weights for accuracy by <strong>minimizing the error by taking the derivative of the error with respect to every weight</strong> in the neural network. The derivative is then subtracted from its corresponding weight. This process is repeated until a desired accuracy has been reached. The neural network now acts as a function where if you give it an input, it should give you an accurate output.</p>
<p>If we are using a sigmoid activation function <span class="math notranslate nohighlight">\(f(z) = \frac{1}{1 + e^{-z}}\)</span> for the hidden layer, the process would be:</p>
<div class="math notranslate nohighlight">
\[
f(\vec{E}_{\mbox{train}} \cdot \vec{W} + b_0 w_0) = \frac{1}{1 + e^{-\vec{E}_{train} \cdot \vec{W} - b_0 w_0}},
\]</div>
<p>where
$<span class="math notranslate nohighlight">\(
0 \le f(\vec{E}_{\mbox{train}} \cdot \vec{W} + b_0 w_0) \le 1.
\)</span>$</p>
<p>Usually, the output layer <strong>also</strong> has an activation function since the synapses between the hidden layers and the output also have weights. For a regression problem the common choice is linear since the values are usually unbounded. If working on a classification problem a common choice is softmax. In this case, we use a linear activation function. The mapping from input to output will then take the form:</p>
<div class="math notranslate nohighlight">
\[
\hat{\delta}_{\mbox{pred}} = f_{\mbox{out}} \big( h_{\mbox{out}} \cdot W_{\mbox{out}} + b_{\mbox{out},0} w_{\mbox{out},0} \big)
\]</div>
<p>The whole training process can be summarized by:</p>
<a class="reference internal image-reference" href="../../_images/training_process.png"><img alt="../../_images/training_process.png" src="../../_images/training_process.png" style="width: 750px;" /></a>
<p>We can use mean-square-error for the loss function</p>
<div class="math notranslate nohighlight">
\[
\mbox{L}_{\mbox{MSE}} = \sum^{N}_{i=1} \big( \delta^{(i)}_{\mbox{train}} - \hat{\delta}^{(i)}_{\mbox{pred}} \big)^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\delta}^{(i)}_{\mbox{pred}}\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>th predicted value, <span class="math notranslate nohighlight">\(\delta^{(i)}_{\mbox{train}}\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>th true or expected value, and <span class="math notranslate nohighlight">\(N\)</span> is the length of the dataset. This loss function takes the square of the difference between the predicted and expected values and sums them up. If the error is below an imposed threshold the training process ends and we can move on to evaluating the testing dataset.</p>
<p>We can now evaluate how well the neural network works by giving it unknown data in the form of the testing dataset <span class="math notranslate nohighlight">\(\vec{E}_{\mbox{test}}\)</span> as an input and comparing the predicted phase shifts to the true values <span class="math notranslate nohighlight">\(\vec{\delta}_{\mbox{test}}\)</span>. <strong>No training is done during the evaluation process</strong>. We will obtain some sort of percentage that tells us how well our model did in accurately predicting the values.</p>
<p>If we are content with the accuracy of the model, we can pass in any value of the scattering energy we want to predict and have it output its corresponding phase shift.</p>
</div>
<div class="section" id="multiple-inputs-outputs-hidden-layers">
<h3>Multiple inputs/outputs/hidden layers<a class="headerlink" href="#multiple-inputs-outputs-hidden-layers" title="Permalink to this headline">¶</a></h3>
<p>This can be extended to multiple inputs and outputs. For instance, if we want to train with inputs <span class="math notranslate nohighlight">\(E\)</span> and <span class="math notranslate nohighlight">\(k_0\)</span> and outputs <span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(\tau\)</span>, the process is the same as before only slightly more complicated. Instead of having an <span class="math notranslate nohighlight">\(N \times 1\)</span> vector for the inputs and weights, we will have a <span class="math notranslate nohighlight">\(N \times 2\)</span> matrix. In addition, our outputs will also be a <span class="math notranslate nohighlight">\(N \times 2\)</span> matrix.</p>
<p>In the case of multiple hidden layers, the process is the same as for the case with one input and one output, only the neurons of the hidden layers will all be connected and they will all have synapses with their own weights and their own (can be different) activation function.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "furnstahl/Physics-8820",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks/Machine_learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../../content/Machine_learning/lecture_23.html" title="previous page"><span class="section-number">9.3. </span>Lecture 23</a>
    <a class='right-next' id="next-link" href="Forssen_tif285_NeuralNet.html" title="next page"><span class="section-number">9.5. </span>Neural networks</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dick Furnstahl<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>